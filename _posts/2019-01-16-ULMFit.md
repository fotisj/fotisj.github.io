---
layout: post
title: ULMFit
---

2018 has seen incredible advances in the development of the technologies which allow tranfer learning from large text corpora to new new domains with much smaller text collections: ULMFit, ELMO, Bert - to name only the most important ones. This post and some in the future will offer a closer look at the architecture of these new models.

The paper by Howard and Ruder [^Howard_Ruder_2018] on ULMFit frames word embeddings as transfer learning. Transfer learning has worked very well with images, where models, which were trained on very large data sets using complex neural nets, were used as starting point for new tasks and achieved very good results. Word embeddings are usually also trained on large corpora, but even if you could achieve better results with them compared to a random initialization of the neural net, if your corpus is even medium sized training the embeddings on your corpus usually gives you better results than using a pretrained model. Howard / Ruder claim it is our lack of knowledge how to train them which is the real problem here. 

First they take up the idea that models trained to solve the task of language models will result in a richer and more complex representation compared to word embeddings like word2vec or Fasttext, which have a much more limited task: predicting the context words given a focus word (or the other way round). I am not sure I understand this claim, because a language model usually gives a probability distribution over your dictionary given the last word or the last words, so the tasks seem to me to be really quite similar. It had already been proposed in 2015 to use first a language model and then do transfer learning by fine tuning on the more specific task as the second step, but even the second step needed millions of documents to work. It is the main claim of this paper that they can reduce the number of instances needed to fine tune drastically: 100 labeled and a collection of 50.000 unlabeled documents. 

The main reason the recent approaches to use language models as pretrained model didn't work are: language models "overfit to small datasets and suffered catastrophic forgetting when fine-tuned with a classifier." (1) So the real problem is to tweak the neural net in such a way, that it does not overfit and does not forget. The network itself is rather simple: it consists of an embedding layer, three plain LSTMs and a final softmax layer. But they break down the training process of the classifier on a small data set into three steps: First a general language model is trained on a large language corpus (they use wikipedia for that), secondly the language model is fine tuned on the small data set, and third, the classifier is trained using the same architecture. Obviously the first 
step needs only to be done once for a language if the corpus is large and general enough.

I. For the first step, the training of the general language model, Howard/Ruder just take a (then) state-of-the-art approach, AWD-LSTM by (Merity et al. 2017)[^Merity_2017]. 

II. The second step, fine tuning the language model on the domain data, is a bit more complex, because here they introduce their strategies to 
avoid overfitting and especially to stop forgetting. Two methods are proposed to achieve this: (1) *discriminative fine-tuning* and (2) *slanted triangular
learning rates*.

(1) *discriminative fine-tuning*
The general idea is quite simple: It is well-known that in neural networks the first layers represent very basic aspects of the data, while later layers 
represent more complex features. But even if our domain data is quite specific, it is very probable that the basic aspects, in language things like part 
of speech, stays the same, while the semantics may be changed in the context of the domain. The idea now is to use different learning rates for the 
levels of the network, basically to slow down the learning for the first levels compared to the last. To be exact, they divide the learning rate of 
level l by 2.6 for level l-1 and again for level l-2.

(2) *slanted triangular learning rates*
This is a variation of triangular learning rates which increase and then decrease the learning rate. Slanted triangular learning rates have a short increase and a long decay period. 

III. The third step, the fine-tuning of the classifier, they apply the methods from step II and *gradual unfreezing*, a modified backpropagation through time and a bidirectional language model. The model is also slightly adapted: two linear blocks are added (with batch normalization, dropout and ReLU activation and Softmax). Input is the pooled hidden layer state of the block described above. The also concatenate the hidden state of the last time step of the document with other representations of the hidden state, in order to catch information important for the classification task anywhere in the document. 

Besides discriminative fine-tuning and slanted triangular learning rates they also use *gradual unfreezing*, which follows the same basic intuition, that the last layer should be changed more than the earlier ones. So they first unfreeze the last layer and fine-tune it for one epoch, then the next one fine-tuning both etc. 

The modification of the backpropagation through time for text classification is also motivated by the wish to capture long-range dependencies, which are typically for information in documents relevant for classification tasks. They propose to split the document in fixed-sized batches and initialize the next batch with the information from the previous one.  

The always train forward and backward language models. In step III. they finetune both language models independently and average the predictions of the classifiers. 

The evaluation on six data sets supports their claim that this is sensible approach. Newer approaches like ELMO and Bert have better results, but it seems to me, this paper is still interesting because it shows how a few basic insights can drive the process of adapting the modeling successfully in this field.

[^Merity_2017]: S. Merity, N. Keskar, and R. Socher. Regularizing and Optimizing LSTM Language Models. [arXiv 7.8.2017](https://arxiv.org/abs/1708.02182). 
[^Howard_Ruder_2018]: J. Howard, S. Ruder: Universal Language Model Fine-tuning for Text Classification. [arXiv 23.5.2018](https://arxiv.org/pdf/1801.06146).