---
layout: post
title: Elmo
---

The paper on *Deep contextualized word representations* by Peters et al. was prepublished March 2018 and won the best paper award on XXX. The basic approach has some similarity to that of ULMFit: the trainings task for the neural net is also a language model and they also emphasize the fact that the first layers 
of a neural net is handling the more 


Our representations differ from traditional word
type embeddings in that each token is assigned a
representation that is a function of the entire input
sentence.