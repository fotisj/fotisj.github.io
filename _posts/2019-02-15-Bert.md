The name Bert is a bit ambigous, because the authors of the paper describe two different approaches: first, an architecture which allows to solve generic NLP problems by adjusting the last layer to the task. Secondly, a way to train a generic language representation which can be used as input to any task specific architecture. 

Bert is based on two other achievements of Google's nlp team: (1) restructuring the input using the wordpiece approach described in WU et al. 2016. (2) The transformer architecture which replaces recurrent networks (

It splits the tokens from a text into smaller pieces to find the right balance between a character model and a token model - the latter is often plagued by the problem of having many rare tokens. "The wordpiece model is generated using a data-driven approach to maximize the language-model likelihood of the training data, given an evolving word definition. Given a training corpus and a number of desired tokens D, the optimization problem is to select D wordpieces such that the resulting corpus is minimal in the number of wordpieces when segmented according to the chosen wordpiece model." Example from the paper (an underscore _ indicates the beginning of a new word):
Word: Jet makers feud over seat width with big orders at stake
wordpieces: _J et _makers _fe ud _over _seat _width _with _big _orders _at _stake
