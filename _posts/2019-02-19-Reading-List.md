---
layout: post
title: Reading List 
---
The following is my reading list on word embeddings and related material

UlmFit: [General](http://nlp.fast.ai/classification/2018/05/15/introducting-ulmfit.html), [paper](https://arxiv.org/pdf/1801.06146), [code/model](https://github.com/bkj/ulm-basenet) 
Elmo: [General](https://allennlp.org/elmo), [paper](https://arxiv.org/abs/1802.05365), [code/model](https://github.com/allenai/bilm-tf) 
Bert (Google): [General](https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html), [paper](https://arxiv.org/abs/1810.04805), [code/model](https://github.com/google-research/bert)

Language modeling related to word embeddings

MT-DNN (Microsoft):  [General](https://syncedreview.com/2019/02/15/microsofts-new-mt-dnn-outperforms-google-bert/), [paper](https://arxiv.org/pdf/1901.11504.pdf), code/model (not yet published)
GPT-2 (OpenAI): [General](https://blog.openai.com/better-language-models/), [paper](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), [code/model](https://github.com/openai/gpt-2) (model not published)

